#åˆ¶ä½œä¸€ä¸ªèŠå¤©ç•Œé¢
#è§£å†³èŠå¤©ç•Œé¢ä¸èƒ½æ¸²æŸ“ä»¥å¾€æ—§å¯¹è¯ä¿¡æ¯
import streamlit as st
#langchain è°ƒç”¨å¤§æ¨¡å‹ï¼Œå¯¼å…¥langchainçš„ä»£ç 
from langchain_openai import ChatOpenAI

#æ„å»ºä¸€ä¸ªå¤§æ¨¡å‹ --æ™ºè°±å…¬å¸æä¾›çš„å¤§æ¨¡å‹
model = ChatOpenAI(
    temperature=0.8,#æ¸©åº¦ï¼Œåˆ›æ–°å‹
    model="glm-4-plus",#å¤§æ¨¡å‹çš„åå­—
    base_url="https://open.bigmodel.cn/api/paas/v4/", #å¤§æ¨¡å‹çš„åœ°å€
    api_key="18f034b2063d0e936bb7eb77170e435f.4dMoHATYmo6BdsH8"#è´¦å·ä¿¡æ¯
)
st.title("AI demoå°ç¨‹åºğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘")
if "cache" not in st.session_state:
    st.session_state.cache = []
else:
    for message in st.session_state.cache:
        with st.chat_message(message['role']):
            st.write(message["content"])



#åˆ›å»ºä¸€ä¸ªèŠå¤©æ¡†
problem = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜")
#åˆ¤æ–­ç”¨æˆ·æœ‰æ²¡æœ‰è¾“å…¥é—®é¢˜
if problem:
    # 1å°†ç”¨æˆ·çš„é—®é¢˜è¾“å‡ºåˆ°ç•Œé¢ï¼Œä»¥ç”¨æˆ·çš„è§’è‰²è¾“å‡º
    with st.chat_message("user"):
        st.write(problem)
    st.session_state.cache.append({"role":"user","content":problem})

    #2è°ƒç”¨å¤§æ¨¡å‹å›ç­”é—®é¢˜
    result = model.invoke(problem)
    #3.å°†å¤§æ¨¡å‹å›ç­”çš„é—®é¢˜è¾“å‡ºåˆ°ç•Œé¢
    with st.chat_message("assistant"):
        st.write(result.content)
    st.session_state.cache.append({"role": "assistant", "content": result.content})